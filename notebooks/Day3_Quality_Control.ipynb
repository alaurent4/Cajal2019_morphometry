{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import os.path as op\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nilearn.plotting.img_plotting import plot_anat\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 3. Processing with Nipype and quality control of the sample MRiShare dataset\n",
    "\n",
    "One of the most important yet least-standardized procedure is the **quality control** of your data. Every lab/researchers have their own method to check the quality of the acquired data and/or processing to make sure they are measuring what they intend to measure. While there is often no clear guideline about what should be checked, since it depends on the modality and processing, it can be classified into the two broad categories.\n",
    "\n",
    "1. QC on raw acquired data\n",
    "    * Does the image have intended FOV?\n",
    "    * Is there significant artefact/noise?\n",
    "    * Are there any abnormalities in the brain (incidental findings)?\n",
    "    --> If any problem is found, either exclude subject/data or keep them and see.\n",
    "    \n",
    "    \n",
    "2. QC on processed data\n",
    "    * Did processing go as intended?\n",
    "    e.g. Skull-stripping, registration, tissue segmentation...\n",
    "    --> If any problem is found, either exclude subject/data or modify the processing steps to resolve the issue.\n",
    "    \n",
    "The most important thing is to **look at your data** systematically, and save the results of any QC check you do in a spreadsheet.\n",
    "\n",
    "In addition to checking your data visually one by one, there are various QC metrics you can collect to find any outliers. For morphometric studies, the morphometric values themselves should be checked for the presence of any outliers. When you find outliers, you can go back to the image with outlier values to decide whether something went wrong in the processing or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Systematically checking the individual images\n",
    "\n",
    "You can use the viewer of your choice to check all the images you have (both the raw images and after some processing), and that's what many labs do. But to make it more efficient, you can include a node in your pipeline that will take an image as input and produce a picture (saved as png file, for example) for every subject and for each processing that should be checked.\n",
    "\n",
    "In ABACI pipeline for MRiShare, we have many such nodes, and we also have a custom script to generate a web html pages that gather generated png files for viewing.\n",
    "\n",
    "Since you learned to create a basic pipeline yesterday, let's start by creating a simple workflow that does the following:\n",
    "\n",
    "1. Coregister FLAIR image to T1\n",
    "2. Skullstrip FLAIR using BET\n",
    "3. Apply mask to T1\n",
    "4. Use FSL FAST\n",
    "5. brainmask QC\n",
    "6. coregistration QC\n",
    "7. tissue segmentation QC\n",
    "8. Datasink to collect important outputs\n",
    "\n",
    "You can use custom interfaces I created in ginnipi_tools.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us create a workflow for anatomical processing\n",
    "\n",
    "Initialize a workflow called `'anat_processing'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nipype import Workflow, Node\n",
    "\n",
    "# your workflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to create a entire workflow that does the procedure on multiple subjects, we could wrap each of our node into a MapNode.\n",
    "\n",
    "<img src=\"../resources/images/mapnode.png\"  width=\"325\">\n",
    "\n",
    "We are going to use the *iterables* argument at the very beginning of the workflow on a node looping on the input subjects. Consequently, the entire procedure will be applied on each subject without using MapNode on each algorithm.\n",
    "\n",
    "<img src=\"../resources/images/iterables.png\"  width=\"240\">\n",
    "\n",
    "We are going to use a special function called IdentityInterface. Here is the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nipype import IdentityInterface\n",
    "\n",
    "subject_id_list = [\"SHARE0001\", \"SHARE0002\"] # change this list of subjects to add more\n",
    "\n",
    "# subject node\n",
    "subjects = Node(IdentityInterface(fields=['subject_id']), name=\"subjects\")\n",
    "subjects.iterables = ('subject_id', subject_id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, unlike yesterday, you are going to work with both T1 and FLAIR images. Create a DataGrabber node to go get the images we need in `/data/ro_formateur/mrishare` . You may of course refer to the Day 2 notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grab T1 and FLAIR images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have your images, you may register FLAIR images on T1 images for each subject. Although these data are of good quality, it is mandatory to register the extra modality on the reference one due to potential head movements.\n",
    "\n",
    "FLIRT algorithm we already used is meant to do rigid registration. Create a Node that wraps this algorithm. We will import the actual function for you. Remember that you can run `FLIRT.help()` to see the mandatory inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nipype.interfaces.fsl import FLIRT\n",
    "\n",
    "# FLAIR registration over T1 modality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now make a skull-stripping with the `BET` function from FSL. We will do a comparison. This time, we will be applying the procedure on the T1 and FLAIR images. Thus, create two wrapping BET Nodes that perform skull-stripping with the `f` argument set to 0.45 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nipype.interfaces.fsl import BET\n",
    "\n",
    "# BET on T1\n",
    "\n",
    "# BET on FLAIR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now have them run both and check the results with Freeview. What can you observe about the efficency of these two procedures ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bet_T1.run()\n",
    "bet_FLAIR.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use only one of these from now on.\n",
    "\n",
    "We are going to use ApplyMask. As a reminder, this function applies a binary mask to extract voxels on an image. In your opinion, what will be the usage here ? And which benefits could we draw from that application ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nipype.interfaces.fsl import ApplyMask\n",
    "\n",
    "# ApplyMask on ???\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tissue segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the major points of the anatomical processing is to perform a tissue segmentation so we can have different volumetry measures of the brain. There are many different procedures and softwares, among them Freesurfer, SPM. But we will focus here once again on an FSL tool.\n",
    "\n",
    "FAST algorithm segments an image of the brain into different tissue types (Grey Matter, White Matter, CSF, etc.), while also correcting for spatial intensity variations (intensity inhomogeneities). The whole process is fully automated and can also produce a bias field-corrected input image and a probabilistic volume tissue segmentation.\n",
    "\n",
    "FAST demands some preprocessing before being able to use. You can have a look at the documentation.\n",
    "\n",
    "https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FAST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nipype.interfaces.fsl import FAST\n",
    "\n",
    "# tissue segmentation with FAST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality control images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These custom functions you will execute are meant to do some quality control on the images coming out of the different procedures.\n",
    "\n",
    "`CoregQC` is meant to check FLAIR registration.\n",
    "\n",
    "`MaskOverlayQCplot` is meant to overlay th brain mask over the original image.\n",
    "\n",
    "`VbmQCplot` may be used to check the grey matter and white matter tissue maps mostly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ginnipi_tools.interfaces.custom import MaskOverlayQCplot, CoregQC, VbmQCplot\n",
    "\n",
    "# CoregQC node : FLAIR registration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MaskOverlayQCplot node : brain mask check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# VbmQCplot node : tissue maps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display some it in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to display png files\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "img=mpimg.imread('/path/to/the/image.png')\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataSink\n",
    "\n",
    "The DataSink interface can be used to extract components from the many different locations of your pipeline. When your workflow is complex and contains many nodes, the result images you may be interested in are scattered. Creating a DataSink allows you to store the images of interest in one folder by subject.\n",
    "\n",
    "We would like to store the original T1, the coregistered FLAIR, the binary brain mask, the three different tissue maps and the png files from the quality control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nipype.interfaces.io import DataSink\n",
    "\n",
    "sink = Node(DataSink(), name='results')\n",
    "sink.inputs.base_directory = '/path/to/output'\n",
    "\n",
    "# to create one folder per subject\n",
    "wf.connect(subjects, 'subject_id', sink, 'container')\n",
    "\n",
    "# you still have to connect all the nodes whose results you want to get\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freesurfer QC\n",
    "\n",
    "One useful tool, in particular for checking **Freesurfer** processed results, is called visualQC (https://raamana.github.io/visualqc/). We will try this out on the processed Freesurfer data for the selected MRiShare subjects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fs_dat_dir = \"/data/rw_eleves/Cajal-Morphometry2019/derived_mrishare/freesurfer/\"\n",
    "sample_dat_dir = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tool has to be used outside of the notebook to be able to use its interacive interface properly. Open a terminal, go to Cajal2019_morphometry folder, then type the following:\n",
    "\n",
    "```bash\n",
    "visualqc_freesurfer -f /data/rw_eleves/Cajal-Morphometry2019/copy/freesurfer/ -o visQCtest -old\n",
    "```\n",
    "\n",
    "I have run this for you once already so that you see the output folder 'visQCtest'. The first time you run it, it creates and saves a series of snapshot useful for reviewing the freesurfer output for every subject in the freesurfer subjects dir you specified.\n",
    "\n",
    "You can also specify a specific set of subjects to review by providing a text file with subject id like below.\n",
    "\n",
    "```bash\n",
    "visualqc_freesurfer -i data/simple_sublist.txt -f /data/rw_eleves/Cajal-Morphometry2019/copy/freesurfer/ -o visQCtest -old\n",
    "```\n",
    "\n",
    "Once it creates the necessary snapshots, executing the same command will trigger the interactive viewer where you check individual images. You need to rate at least one subject to be able to press 'Quit' to exit the interface.\n",
    "\n",
    "There are several options for what/how you can review. Try out a few examples from https://raamana.github.io/visualqc/examples_freesurfer.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the distribution and outliers for QC and other metrics\n",
    "\n",
    "Any metric you collect as part of the analysis should be checked for any outliers. In addition, there are many other QC metrics proposed for structural and functional image processing, as listed here (http://preprocessed-connectomes-project.org/quality-assessment-protocol/).\n",
    "\n",
    "Here we will use both the morphometric data and some of the selected QC metric we computed for MRiShare subjects to see if there is any problematic subjects.\n",
    "\n",
    "**QC metrics**\n",
    "\n",
    "1) Tissue SNR\n",
    "\n",
    "    * computed as mean/sd in each tissue in each compartments\n",
    "\n",
    "2) Tissue CNR\n",
    "\n",
    "    * for T1 stats, WMGM (WM mean/GM mean)and GMCSF (GM mean/CSF mean)\n",
    "    * for T2flair stats, GMWM (GM mean/ WM mean) and GMCSF (GM mean/CSF mean)\n",
    "    \n",
    "3) Coregistration cost function\n",
    "\n",
    "**Morphometrics**\n",
    "\n",
    "1) SPM GM, WM, CSF volume\n",
    "\n",
    "2) Freesurfer global metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qc_dat = pd.read_csv(op.join(sample_dat_dir, 'sample_qc.csv'))\n",
    "qc_dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "morph_dat = pd.read_csv(op.join(sample_dat_dir, 'sample_mrishare_morphometry.csv'))\n",
    "morph_dat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several python visualization packages that allows you to interactively inspect your data.\n",
    "\n",
    "Perhaps one of the most easiest one to use is **plotly_express** (https://medium.com/plotly/introducing-plotly-express-808df010143d), as demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(qc_dat, x=\"SPM_GM_hemiR_SNR\", y=\"SPM_GM_hemiL_SNR\", hover_name=\"mrishare_id\", marginal_y=\"violin\",\n",
    "           marginal_x=\"box\", trendline=\"ols\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hippRL_vols = morph_dat[['mrishare_id', 'FS6_gm_R_hippo', 'FS6_gm_L_hippo']]\n",
    "hippRL_vols.set_index('mrishare_id', inplace = True)\n",
    "stacked_hippRL = hippRL_vols.stack()\n",
    "stacked_hippRL = stacked_hippRL.reset_index()\n",
    "stacked_hippRL.columns = ['mrishare_id', 'measure', 'volume']\n",
    "stacked_hippRL.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = px.violin(stacked_hippRL, y=\"volume\", color=\"measure\", box=True, points=\"all\", hover_data=stacked_hippRL.columns)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated files can be saved as a web html or image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import plotly.io as pio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pio.write_html(fig, 'FS6_hipp_dist.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another nice python package for interactive plotting is **bokeh** (https://bokeh.pydata.org/en/latest/index.html). But it's slightly more involved, so here we share you some functions we created in our lab to have two types of plots we use for QC check:\n",
    "\n",
    "1. Distribution plot to check for outliers\n",
    "2. Pairplots to check for asymmetry\n",
    "\n",
    "Each type of plot can be created with *plot_hist_box* and *pairplots_by_region* functions in ginnipi_tools package. \n",
    "\n",
    "Here is the usage example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ginnipi_tools.toolbox.plotting_tools import plot_hist_box, pairplots_by_region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we just need to set the column with subject id as \"index\" in DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "morph_dat2 = morph_dat.set_index('mrishare_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the first function to summarize the SPM volumes, and save as 'SPM_vol.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_hist_box(morph_dat2,\n",
    "              measure_name='volume',\n",
    "              col_groupname='tissue',\n",
    "              cols_to_plot=['SPM_GM_Volume','SPM_WM_Volume', 'SPM_CSF_Volume'],\n",
    "              title='Distribution of SPM volumes',\n",
    "              out_html='SPM_vol.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use the pairplots_by_region function to summarize the asymmetry of hipp volumes and save as 'FS6_hipp_asym.html'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pairplots_by_region(morph_dat2,\n",
    "                    measure_name='volume',\n",
    "                    col1='FS6_gm_R_hippo',\n",
    "                    col2='FS6_gm_L_hippo',\n",
    "                    plot_size=(400, 400),\n",
    "                    bgcolor=\"white\",\n",
    "                    title='Hippocampal GM R vs L',\n",
    "                    out_html='FS6_hipp_asym.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than saving each plot separately, you can use bokeh layout tools (https://bokeh.pydata.org/en/latest/docs/user_guide/layout.html) to combine multiple plots and save them as one file. Below is some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bokeh.layouts import gridplot, row, column\n",
    "from bokeh.models import Div, Spacer\n",
    "import bokeh.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dist_plot = plot_hist_box(morph_dat2,\n",
    "                          measure_name='volume',\n",
    "                          col_groupname='tissue',\n",
    "                          cols_to_plot=['SPM_GM_Volume','SPM_WM_Volume', 'SPM_CSF_Volume'],\n",
    "                          title='Distribution of SPM volumes')\n",
    "pplot = pairplots_by_region(morph_dat2,\n",
    "                            measure_name='volume',\n",
    "                            col1='FS6_gm_R_hippo',\n",
    "                            col2='FS6_gm_L_hippo',\n",
    "                            plot_size=(600, 600),\n",
    "                            bgcolor=\"white\",\n",
    "                            title='Hippocampal GM R vs L')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_plot = column([dist_plot, Spacer(height=100), pplot])\n",
    "bokeh.io.save(combined_plot,\n",
    "              'combined_plot.html',\n",
    "              title='test of combining plots')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

