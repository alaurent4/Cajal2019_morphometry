{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/padawan/anaconda3/envs/cajal2019/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import os.path as op\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nilearn.plotting.img_plotting import plot_anat\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 3. Processing with Nipype and quality control of the sample MRiShare dataset\n",
    "\n",
    "One of the most important yet least-standardized procedure is the **quality control** of your data. Every lab/researchers have their own method to check the quality of the acquired data and/or processing to make sure they are measuring what they intend to measure. While there is often no clear guideline about what should be checked, since it depends on the modality and processing, it can be classified into the two broad categories.\n",
    "\n",
    "1. QC on raw acquired data\n",
    "    * Does the image have intended FOV?\n",
    "    * Is there significant artefact/noise?\n",
    "    * Are there any abnormalities in the brain (incidental findings)?\n",
    "    --> If any problem is found, either exclude subject/data or keep them and see.\n",
    "    \n",
    "    \n",
    "2. QC on processed data\n",
    "    * Did processing go as intended?\n",
    "    e.g. Skull-stripping, registration, tissue segmentation...\n",
    "    --> If any problem is found, either exclude subject/data or modify the processing steps to resolve the issue.\n",
    "    \n",
    "The most important thing is to **look at your data** systematically, and save the results of any QC check you do in a spreadsheet.\n",
    "\n",
    "In addition to checking your data visually one by one, there are various QC metrics you can collect to find any outliers. For morphometric studies, the morphometric values themselves should be checked for the presence of any outliers. When you find outliers, you can go back to the image with outlier values to decide whether something went wrong in the processing or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Systematically checking the individual images\n",
    "\n",
    "You can use the viewer of your choice to check all the images you have (both the raw images and after some processing), and that's what many labs do. But to make it more efficient, you can include a node in your pipeline that will take an image as input and produce a picture (saved as png file, for example) for every subject and for each processing that should be checked.\n",
    "\n",
    "In ABACI pipeline for MRiShare, we have many such nodes, and we also have a custom script to generate a web html pages that gather generated png files for viewing.\n",
    "\n",
    "Since you learned to create a basic pipeline yesterday, let's start by creating a simple workflow that does the following:\n",
    "\n",
    "1. Coregister FLAIR image to T1\n",
    "2. Skullstrip FLAIR using BET\n",
    "3. Apply mask to T1\n",
    "4. Use FSL FAST\n",
    "5. brainmask QC\n",
    "6. coregistration QC\n",
    "7. tissue segmentation QC\n",
    "8. Datasink to collect important outputs\n",
    "\n",
    "You can use custom interfaces I created in ginnipi_tools.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us create a workflow for anatomical processing\n",
    "\n",
    "Initialize a workflow called `'anat_processing'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "dat_dir = '/data/ro_formateur/mrishare/'\n",
    "sub_glob = sorted(glob.glob(op.join(dat_dir, 'SHARE*')))\n",
    "sub_dirs = [op.basename(p) for p in sub_glob]\n",
    "sub_dirs = sub_dirs[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/padawan/Cajal2019_morphometry/Day_3’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "from nipype import Workflow, Node, DataGrabber\n",
    "from nipype.interfaces.fsl import FLIRT, BET, ApplyMask, FAST\n",
    "import nipype.interfaces.io as nio\n",
    "\n",
    "# Make a new directory to save results\n",
    "! mkdir /home/padawan/Cajal2019_morphometry/Day_3\n",
    "\n",
    "# your workflow\n",
    "my_wf = Workflow(name=\"anat_processing\", base_dir=\"/home/padawan/Day_3\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to create a entire workflow that does the procedure on multiple subjects, we could wrap each of our node into a MapNode.\n",
    "\n",
    "<img src=\"../resources/images/mapnode.png\"  width=\"325\">\n",
    "\n",
    "We are going to use the *iterables* argument at the very beginning of the workflow on a node looping on the input subjects. Consequently, the entire procedure will be applied on each subject without using MapNode on each algorithm.\n",
    "\n",
    "<img src=\"../resources/images/iterables.png\"  width=\"240\">\n",
    "\n",
    "We are going to use a special function called IdentityInterface. Here is the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "! export FSLOUTPUTTYPE=NIFTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nipype import IdentityInterface\n",
    "\n",
    "# subject node\n",
    "subjects = Node(IdentityInterface(fields=['subject_id']), name=\"subjects\")\n",
    "subjects.iterables = ('subject_id', sub_dirs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, unlike yesterday, you are going to work with both T1 and FLAIR images. Create a DataGrabber node to go get the images we need in `/data/ro_formateur/mrishare` . You may of course refer to the Day 2 notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab T1 and FLAIR images\n",
    "\n",
    "# Node to grab the data\n",
    "dg_node = Node(DataGrabber(infields=['subject_id'],\n",
    "                      outfields=['T1', 'FLAIR']),\n",
    "          name='datagrabber')\n",
    "\n",
    "# Location of the dataset folder\n",
    "dg_node.inputs.base_directory = '/data/ro_formateur/mrishare/'\n",
    "dg_node.inputs.template_args = {'T1': [['subject_id', 'subject_id', 'T1w']],\n",
    "                           'FLAIR': [['subject_id', 'subject_id', 'FLAIR']]}\n",
    "dg_node.inputs.template = '%s/anat/%s_%s.nii.gz'\n",
    "dg_node.inputs.sort_filelist = True\n",
    "\n",
    "my_wf.connect(subjects, \"subject_id\", dg_node, \"subject_id\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have your images, you may register FLAIR images on T1 images for each subject. Although these data are of good quality, it is mandatory to register the extra modality on the reference one due to potential head movements.\n",
    "\n",
    "FLIRT algorithm we already used is meant to do rigid registration. Create a Node that wraps this algorithm. We will import the actual function for you. Remember that you can run `FLIRT.help()` to see the mandatory inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190911-18:58:58,109 nipype.interface WARNING:\n",
      "\t FSLOUTPUTTYPE environment variable is not set. Setting FSLOUTPUTTYPE=NIFTI\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# FLAIR registration over T1 modality\n",
    "\n",
    "# Node to coregister FLAIR to T1 for each person\n",
    "Coreg_node = Node(FLIRT(),name = 'Coreg')\n",
    "\n",
    "my_wf.connect(dg_node, \"FLAIR\", Coreg_node, \"in_file\")\n",
    "my_wf.connect(dg_node, \"T1\", Coreg_node, \"reference\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now make a skull-stripping with the `BET` function from FSL. We will do a comparison. This time, we will be applying the procedure on the T1 and FLAIR images. Thus, create two wrapping BET Nodes that perform skull-stripping with the `f` argument set to 0.45 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190911-18:58:58,421 nipype.interface WARNING:\n",
      "\t FSLOUTPUTTYPE environment variable is not set. Setting FSLOUTPUTTYPE=NIFTI\n",
      "190911-18:58:58,422 nipype.interface WARNING:\n",
      "\t FSLOUTPUTTYPE environment variable is not set. Setting FSLOUTPUTTYPE=NIFTI\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# BET on T1\n",
    "betT1_node = Node(BET(frac = 0.5), name = \"betT1\")\n",
    "betT1_node.inputs.mask = True\n",
    "\n",
    "my_wf.connect(dg_node, \"T1\", betT1_node, \"in_file\")\n",
    "\n",
    "\n",
    "# BET on FLAIR\n",
    "betFLAIR_node = Node(BET(frac = 0.5), name = \"betFl\")\n",
    "betFLAIR_node.inputs.mask = True\n",
    "\n",
    "my_wf.connect(Coreg_node, \"out_file\", betFLAIR_node, \"in_file\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now have them run both and check the results with Freeview. What can you observe about the efficency of these two procedures ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use only one of these from now on.\n",
    "\n",
    "We are going to use ApplyMask. As a reminder, this function applies a binary mask to extract voxels on an image. In your opinion, what will be the usage here ? And which benefits could we draw from that application ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190911-18:58:59,364 nipype.interface WARNING:\n",
      "\t FSLOUTPUTTYPE environment variable is not set. Setting FSLOUTPUTTYPE=NIFTI\n"
     ]
    }
   ],
   "source": [
    "from nipype.interfaces.fsl import ApplyMask\n",
    "\n",
    "# ApplyMask on T1 from FLAIR BET\n",
    "\n",
    "mask_node = Node(ApplyMask(), name = \"mask\")\n",
    "my_wf.connect(betFLAIR_node, \"mask_file\", mask_node, \"mask_file\")\n",
    "my_wf.connect(dg_node, \"T1\", mask_node, \"in_file\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tissue segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the major points of the anatomical processing is to perform a tissue segmentation so we can have different volumetry measures of the brain. There are many different procedures and softwares, among them Freesurfer, SPM. But we will focus here once again on an FSL tool.\n",
    "\n",
    "FAST algorithm segments an image of the brain into different tissue types (Grey Matter, White Matter, CSF, etc.), while also correcting for spatial intensity variations (intensity inhomogeneities). The whole process is fully automated and can also produce a bias field-corrected input image and a probabilistic volume tissue segmentation.\n",
    "\n",
    "FAST demands some preprocessing before being able to use. You can have a look at the documentation.\n",
    "\n",
    "https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FAST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190911-18:58:59,941 nipype.interface WARNING:\n",
      "\t FSLOUTPUTTYPE environment variable is not set. Setting FSLOUTPUTTYPE=NIFTI\n"
     ]
    }
   ],
   "source": [
    "from nipype.interfaces.fsl import FAST\n",
    "\n",
    "# tissue segmentation with FAST\n",
    "FAST_node = Node(FAST(), name = \"FAST\")\n",
    "FAST_node.inputs.img_type = 1\n",
    "FAST_node.inputs.number_classes = 3\n",
    "FAST_node.inputs.output_biascorrected = True\n",
    "FAST_node.inputs.output_biasfield = True\n",
    "FAST_node.inputs.probability_maps = True\n",
    "\n",
    "my_wf.connect(mask_node, \"out_file\", FAST_node, \"in_files\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality control images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wraps the executable command ``coreg_QC.sh``.\n",
      "\n",
      "Creates multi-slice axial plot with Slicer showing coregistation IsoContours\n",
      "and also computes FLIRT costfunction.\n",
      "\n",
      "Inputs::\n",
      "\n",
      "        [Mandatory]\n",
      "        coreg_im_file: (a unicode string)\n",
      "                Coregistered image to be plotted\n",
      "                argument: ``%s``, position: 1\n",
      "        ref_file: (a unicode string)\n",
      "                Ref image file (typically T1 brain) whose edge will be shown\n",
      "                argument: ``%s``, position: 2\n",
      "        mask_file: (a unicode string)\n",
      "                Brain mask\n",
      "                argument: ``%s``, position: 3\n",
      "        out_basename: (a unicode string)\n",
      "                Output basename for png and txt\n",
      "                argument: ``%s``, position: 4\n",
      "\n",
      "        [Optional]\n",
      "        args: (a unicode string)\n",
      "                Additional parameters to the command\n",
      "                argument: ``%s``\n",
      "        environ: (a dictionary with keys which are a bytes or None or a value\n",
      "                  of class 'str' and with values which are a bytes or None or a\n",
      "                  value of class 'str', nipype default value: {})\n",
      "                Environment variables\n",
      "\n",
      "Outputs::\n",
      "\n",
      "        out_plot: (a pathlike object or string representing an existing file)\n",
      "        out_txt: (a pathlike object or string representing an existing file)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ginnipi_tools.interfaces.custom import MaskOverlayQCplot, CoregQC, VbmQCplot\n",
    "CoregQC.help()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These custom functions you will execute are meant to do some quality control on the images coming out of the different procedures.\n",
    "\n",
    "`CoregQC` is meant to check FLAIR registration.\n",
    "\n",
    "`MaskOverlayQCplot` is meant to overlay th brain mask over the original image.\n",
    "\n",
    "`VbmQCplot` may be used to check the grey matter and white matter tissue maps mostly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ginnipi_tools.interfaces.custom import MaskOverlayQCplot, CoregQC, VbmQCplot\n",
    "\n",
    "# CoregQC node : FLAIR registration\n",
    "#Coreg_QC_node = Node(CoregQC(), name = \"Coreg_QC\")\n",
    "\n",
    "#my_wf.connect(Coreg_node, \"out_file\", Coreg_QC_node, \"coreg_im_file\")\n",
    "#my_wf.connect(dg_node, \"T1\", Coreg_QC_node, \"ref_file\")\n",
    "#my_wf.connect(betFLAIR_node, \"mask_file\", Coreg_QC_node, \"mask_file\") \n",
    "#my_wf.connect(subjects, \"subject_id\", Coreg_QC_node, \"out_basename\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MaskOverlayQCplot node : brain mask check\n",
    "#MaskOverlay_node = Node(MaskOverlayQCplot(), name = \"mask_overlay_QC\")\n",
    "#MaskOverlay_node.inputs.transparency = 1\n",
    "\n",
    "#my_wf.connect(Coreg_node, \"out_file\", MaskOverlay_node, \"bg_im_file\")\n",
    "#my_wf.connect(betFLAIR_node, \"mask_file\", MaskOverlay_node, \"mask_file\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def grab_gm(l):\n",
    "#    return l[1] \n",
    "\n",
    "#def grab_wm(l):\n",
    " #   return l[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VbmQCplot node : tissue maps\n",
    "#VbmQCplot_node = Node(VbmQCplot(), name = \"VbmQC\")\n",
    "\n",
    "\n",
    "\n",
    "#my_wf.connect(dg_node, \"T1\", VbmQCplot_node, \"bg_file\")\n",
    "#my_wf.connect(FAST_node, (\"tissue_class_files\",grab_gm), VbmQCplot_node, \"gm_file\")\n",
    "#my_wf.connect(FAST_node, (\"tissue_class_files\",grab_wm), VbmQCplot_node, \"wm_file\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wraps the executable command ``create_vbm_QC_images.sh``.\n",
      "\n",
      "Plots mid-saggital slices per frame for viewing slice-to-volume\n",
      "movement artefacts in DWI data.\n",
      "\n",
      "Inputs::\n",
      "\n",
      "        [Mandatory]\n",
      "        bg_file: (a unicode string)\n",
      "                background image file to plot\n",
      "                argument: ``%s``, position: 1\n",
      "        gm_file: (a unicode string)\n",
      "                GM image file to overlay\n",
      "                argument: ``%s``, position: 2\n",
      "        wm_file: (a unicode string)\n",
      "                WM image file to overlay\n",
      "                argument: ``%s``, position: 3\n",
      "\n",
      "        [Optional]\n",
      "        use_jet: (0 or 1, nipype default value: 0)\n",
      "                Use jet colorscheme\n",
      "                argument: ``%d``, position: 4\n",
      "        out_name: (a unicode string, nipype default value: VBM)\n",
      "                Prefix to the output png filename\n",
      "                argument: ``%s``, position: 5\n",
      "        bg_max: (a float, nipype default value: 99.9)\n",
      "                Optionally specifies the bg img intensity range as a percentile\n",
      "                argument: ``%.3f``, position: 6\n",
      "        gm_max: (a float, nipype default value: 1.0)\n",
      "                Optionally specifies the max val of overlaying GM\n",
      "                argument: ``%.3f``, position: 7\n",
      "        wm_max: (a float, nipype default value: 1.0)\n",
      "                Optionally specifies the max val of overlaying WM\n",
      "                argument: ``%.3f``, position: 8\n",
      "        args: (a unicode string)\n",
      "                Additional parameters to the command\n",
      "                argument: ``%s``\n",
      "        environ: (a dictionary with keys which are a bytes or None or a value\n",
      "                  of class 'str' and with values which are a bytes or None or a\n",
      "                  value of class 'str', nipype default value: {})\n",
      "                Environment variables\n",
      "\n",
      "Outputs::\n",
      "\n",
      "        output_images: (a list of items which are a pathlike object or string\n",
      "                  representing an existing file)\n",
      "                QC images in png format\n",
      "\n",
      "Wraps the executable command ``mask_overlay_QC_images.sh``.\n",
      "\n",
      "Creates multi-slice axial plot with Slicer showing mask overlaied on\n",
      "background image.\n",
      "\n",
      "Inputs::\n",
      "\n",
      "        [Mandatory]\n",
      "        bg_im_file: (a unicode string)\n",
      "                Background ref image file (typically T1 brain)\n",
      "                argument: ``%s``, position: 1\n",
      "        mask_file: (a unicode string)\n",
      "                Brain mask\n",
      "                argument: ``%s``, position: 2\n",
      "        transparency: (0 or 1)\n",
      "                Set transparency (0: solid, 1:transparent)\n",
      "                argument: ``%d``, position: 3\n",
      "\n",
      "        [Optional]\n",
      "        out_file: (a unicode string)\n",
      "                Output png filename\n",
      "                argument: ``%s``, position: 4\n",
      "        bg_max: (a float)\n",
      "                Optionally specifies the bg img intensity range as a percentile\n",
      "                argument: ``%.3f``, position: 5\n",
      "        args: (a unicode string)\n",
      "                Additional parameters to the command\n",
      "                argument: ``%s``\n",
      "        environ: (a dictionary with keys which are a bytes or None or a value\n",
      "                  of class 'str' and with values which are a bytes or None or a\n",
      "                  value of class 'str', nipype default value: {})\n",
      "                Environment variables\n",
      "\n",
      "Outputs::\n",
      "\n",
      "        out_file: (a pathlike object or string representing an existing file)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "VbmQCplot.help()\n",
    "MaskOverlayQCplot.help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display some it in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/path/to/the/image.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-426b992d10e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmpimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmpimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/path/to/the/image.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mimgplot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cajal2019/lib/python3.7/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1431\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1434\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/path/to/the/image.png'"
     ]
    }
   ],
   "source": [
    "# to display png files\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "img=mpimg.imread('/path/to/the/image.png')\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataSink\n",
    "\n",
    "The DataSink interface can be used to extract components from the many different locations of your pipeline. When your workflow is complex and contains many nodes, the result images you may be interested in are scattered. Creating a DataSink allows you to store the images of interest in one folder by subject.\n",
    "\n",
    "We would like to store the original T1, the coregistered FLAIR, the binary brain mask, the three different tissue maps and the png files from the quality control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nipype.interfaces.io import DataSink\n",
    "\n",
    "sink = Node(DataSink(), name='sink_result')\n",
    "sink.inputs.base_directory = '/home/padawan/Day_3/data_sink'\n",
    "\n",
    "# to create one folder per subject\n",
    "my_wf.connect(subjects, 'subject_id', sink, 'container')\n",
    "\n",
    "# you still have to connect all the nodes whose results you want to get\n",
    "my_wf.connect(dg_node, \"T1\", sink, \"T1\")\n",
    "my_wf.connect(Coreg_node, \"out_file\", sink, \"Coreg_FLAIR\")\n",
    "my_wf.connect(betFLAIR_node, \"mask_file\", sink, \"Mask\")\n",
    "my_wf.connect(FAST_node,\"tissue_class_files\", sink, \"tissue_files\") \n",
    "#my_wf.connect(Coreg_QC_node, \"out_plot\", sink, \"Coreg_QC_pics\")\n",
    "#my_wf.connect(MaskOverlay_node, \"out_file\", sink, \"MaskOverlayQC_pics\")\n",
    "#my_wf.connect(VbmQCplot_node, \"output_images\", sink, \"VBM_QC_pics\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190911-18:59:34,127 nipype.workflow INFO:\n",
      "\t Workflow anat_processing settings: ['check', 'execution', 'logging', 'monitoring']\n",
      "190911-18:59:34,166 nipype.workflow INFO:\n",
      "\t Running serially.\n",
      "190911-18:59:34,167 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"anat_processing.datagrabber\" in \"/home/padawan/Day_3/anat_processing/_subject_id_SHARE0040/datagrabber\".\n",
      "190911-18:59:34,170 nipype.workflow INFO:\n",
      "\t [Node] Running \"datagrabber\" (\"nipype.interfaces.io.DataGrabber\")\n",
      "190911-18:59:34,175 nipype.workflow INFO:\n",
      "\t [Node] Finished \"anat_processing.datagrabber\".\n",
      "190911-18:59:34,176 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"anat_processing.betT1\" in \"/home/padawan/Day_3/anat_processing/_subject_id_SHARE0040/betT1\".\n",
      "190911-18:59:34,177 nipype.workflow INFO:\n",
      "\t [Node] Cached \"anat_processing.betT1\" - collecting precomputed outputs\n",
      "190911-18:59:34,178 nipype.workflow INFO:\n",
      "\t [Node] \"anat_processing.betT1\" found cached.\n",
      "190911-18:59:34,178 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"anat_processing.Coreg\" in \"/home/padawan/Day_3/anat_processing/_subject_id_SHARE0040/Coreg\".\n",
      "190911-18:59:34,180 nipype.workflow INFO:\n",
      "\t [Node] Cached \"anat_processing.Coreg\" - collecting precomputed outputs\n",
      "190911-18:59:34,181 nipype.workflow INFO:\n",
      "\t [Node] \"anat_processing.Coreg\" found cached.\n",
      "190911-18:59:34,181 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"anat_processing.betFl\" in \"/home/padawan/Day_3/anat_processing/_subject_id_SHARE0040/betFl\".\n",
      "190911-18:59:34,183 nipype.workflow INFO:\n",
      "\t [Node] Cached \"anat_processing.betFl\" - collecting precomputed outputs\n",
      "190911-18:59:34,183 nipype.workflow INFO:\n",
      "\t [Node] \"anat_processing.betFl\" found cached.\n",
      "190911-18:59:34,184 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"anat_processing.mask\" in \"/home/padawan/Day_3/anat_processing/_subject_id_SHARE0040/mask\".\n",
      "190911-18:59:34,186 nipype.workflow INFO:\n",
      "\t [Node] Cached \"anat_processing.mask\" - collecting precomputed outputs\n",
      "190911-18:59:34,186 nipype.workflow INFO:\n",
      "\t [Node] \"anat_processing.mask\" found cached.\n",
      "190911-18:59:34,187 nipype.workflow INFO:\n",
      "\t [Node] Setting-up \"anat_processing.FAST\" in \"/home/padawan/Day_3/anat_processing/_subject_id_SHARE0040/FAST\".\n",
      "190911-18:59:34,190 nipype.workflow INFO:\n",
      "\t [Node] Running \"FAST\" (\"nipype.interfaces.fsl.preprocess.FAST\"), a CommandLine Interface with command:\n",
      "fast -t 1 -n 3 -B -b -p -S 1 /home/padawan/Day_3/anat_processing/_subject_id_SHARE0040/FAST/SHARE0040_T1w_masked.nii\n"
     ]
    }
   ],
   "source": [
    "my_wf.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freesurfer QC\n",
    "\n",
    "One useful tool, in particular for checking **Freesurfer** processed results, is called visualQC (https://raamana.github.io/visualqc/). We will try this out on the processed Freesurfer data for the selected MRiShare subjects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_dat_dir = \"/data/rw_eleves/Cajal-Morphometry2019/derived_mrishare/freesurfer/\"\n",
    "sample_dat_dir = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tool has to be used outside of the notebook to be able to use its interacive interface properly. Open a terminal, go to Cajal2019_morphometry folder, then type the following:\n",
    "\n",
    "```bash\n",
    "visualqc_freesurfer -f /data/rw_eleves/Cajal-Morphometry2019/copy/freesurfer/ -o visQCtest -old\n",
    "```\n",
    "\n",
    "I have run this for you once already so that you see the output folder 'visQCtest'. The first time you run it, it creates and saves a series of snapshot useful for reviewing the freesurfer output for every subject in the freesurfer subjects dir you specified.\n",
    "\n",
    "You can also specify a specific set of subjects to review by providing a text file with subject id like below.\n",
    "\n",
    "```bash\n",
    "visualqc_freesurfer -i data/simple_sublist.txt -f /data/rw_eleves/Cajal-Morphometry2019/copy/freesurfer/ -o visQCtest -old\n",
    "```\n",
    "\n",
    "Once it creates the necessary snapshots, executing the same command will trigger the interactive viewer where you check individual images. You need to rate at least one subject to be able to press 'Quit' to exit the interface.\n",
    "\n",
    "There are several options for what/how you can review. Try out a few examples from https://raamana.github.io/visualqc/examples_freesurfer.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the distribution and outliers for QC and other metrics\n",
    "\n",
    "Any metric you collect as part of the analysis should be checked for any outliers. In addition, there are many other QC metrics proposed for structural and functional image processing, as listed here (http://preprocessed-connectomes-project.org/quality-assessment-protocol/).\n",
    "\n",
    "Here we will use both the morphometric data and some of the selected QC metric we computed for MRiShare subjects to see if there is any problematic subjects.\n",
    "\n",
    "**QC metrics**\n",
    "\n",
    "1) Tissue SNR\n",
    "\n",
    "    * computed as mean/sd in each tissue in each compartments\n",
    "\n",
    "2) Tissue CNR\n",
    "\n",
    "    * for T1 stats, WMGM (WM mean/GM mean)and GMCSF (GM mean/CSF mean)\n",
    "    * for T2flair stats, GMWM (GM mean/ WM mean) and GMCSF (GM mean/CSF mean)\n",
    "    \n",
    "3) Coregistration cost function\n",
    "\n",
    "**Morphometrics**\n",
    "\n",
    "1) SPM GM, WM, CSF volume\n",
    "\n",
    "2) Freesurfer global metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_dat = pd.read_csv(op.join(sample_dat_dir, 'sample_qc.csv'))\n",
    "qc_dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_dat = pd.read_csv(op.join(sample_dat_dir, 'sample_mrishare_morphometry.csv'))\n",
    "morph_dat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several python visualization packages that allows you to interactively inspect your data.\n",
    "\n",
    "Perhaps one of the most easiest one to use is **plotly_express** (https://medium.com/plotly/introducing-plotly-express-808df010143d), as demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(qc_dat, x=\"SPM_GM_hemiR_SNR\", y=\"SPM_GM_hemiL_SNR\", hover_name=\"mrishare_id\", marginal_y=\"violin\",\n",
    "           marginal_x=\"box\", trendline=\"ols\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hippRL_vols = morph_dat[['mrishare_id', 'FS6_gm_R_hippo', 'FS6_gm_L_hippo']]\n",
    "hippRL_vols.set_index('mrishare_id', inplace = True)\n",
    "stacked_hippRL = hippRL_vols.stack()\n",
    "stacked_hippRL = stacked_hippRL.reset_index()\n",
    "stacked_hippRL.columns = ['mrishare_id', 'measure', 'volume']\n",
    "stacked_hippRL.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.violin(stacked_hippRL, y=\"volume\", color=\"measure\", box=True, points=\"all\", hover_data=stacked_hippRL.columns)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated files can be saved as a web html or image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pio.write_html(fig, 'FS6_hipp_dist.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another nice python package for interactive plotting is **bokeh** (https://bokeh.pydata.org/en/latest/index.html). But it's slightly more involved, so here we share you some functions we created in our lab to have two types of plots we use for QC check:\n",
    "\n",
    "1. Distribution plot to check for outliers\n",
    "2. Pairplots to check for asymmetry\n",
    "\n",
    "Each type of plot can be created with *plot_hist_box* and *pairplots_by_region* functions in ginnipi_tools package. \n",
    "\n",
    "Here is the usage example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ginnipi_tools.toolbox.plotting_tools import plot_hist_box, pairplots_by_region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we just need to set the column with subject id as \"index\" in DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_dat2 = morph_dat.set_index('mrishare_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the first function to summarize the SPM volumes, and save as 'SPM_vol.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist_box(morph_dat2,\n",
    "              measure_name='volume',\n",
    "              col_groupname='tissue',\n",
    "              cols_to_plot=['SPM_GM_Volume','SPM_WM_Volume', 'SPM_CSF_Volume'],\n",
    "              title='Distribution of SPM volumes',\n",
    "              out_html='SPM_vol.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use the pairplots_by_region function to summarize the asymmetry of hipp volumes and save as 'FS6_hipp_asym.html'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairplots_by_region(morph_dat2,\n",
    "                    measure_name='volume',\n",
    "                    col1='FS6_gm_R_hippo',\n",
    "                    col2='FS6_gm_L_hippo',\n",
    "                    plot_size=(400, 400),\n",
    "                    bgcolor=\"white\",\n",
    "                    title='Hippocampal GM R vs L',\n",
    "                    out_html='FS6_hipp_asym.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than saving each plot separately, you can use bokeh layout tools (https://bokeh.pydata.org/en/latest/docs/user_guide/layout.html) to combine multiple plots and save them as one file. Below is some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.layouts import gridplot, row, column\n",
    "from bokeh.models import Div, Spacer\n",
    "import bokeh.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_plot = plot_hist_box(morph_dat2,\n",
    "                          measure_name='volume',\n",
    "                          col_groupname='tissue',\n",
    "                          cols_to_plot=['SPM_GM_Volume','SPM_WM_Volume', 'SPM_CSF_Volume'],\n",
    "                          title='Distribution of SPM volumes')\n",
    "pplot = pairplots_by_region(morph_dat2,\n",
    "                            measure_name='volume',\n",
    "                            col1='FS6_gm_R_hippo',\n",
    "                            col2='FS6_gm_L_hippo',\n",
    "                            plot_size=(600, 600),\n",
    "                            bgcolor=\"white\",\n",
    "                            title='Hippocampal GM R vs L')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_plot = column([dist_plot, Spacer(height=100), pplot])\n",
    "bokeh.io.save(combined_plot,\n",
    "              'combined_plot.html',\n",
    "              title='test of combining plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
